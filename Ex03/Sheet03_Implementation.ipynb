{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exercise Sheet 3"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "INTRO"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this exercise, we'll be using a different dataset for spam classification, available at http://archive.ics.uci.edu/ml/datasets/Spambase \n",
      "\n",
      "The data measures frequencies of certain words and characters, as well as information about capital letters in the text.\n",
      "\n",
      "Every column represents a certain attribute. All values are $\\geq0$. We assume they are independent.\n",
      "\n",
      "You will be using the bayes rule to classify messages. To determine the probabilities, two approaches will be used:\n",
      "\n",
      "1) Assuming that all attributes originated from an exponential distribution. We wish to test if this assumption makes sense, and if so, for which attributes.\n",
      "\n",
      "2) Using kernel density estimation to determine a probability density function for each attribute. This should give us with a good estimate of the densities, so we have a basis of comparison\n",
      "\n",
      "You do not need to implement kernel density estimation yourselves. Instead, we'll be using scipy's inbuilt function with the gaussian kernel. Basics about it can be found at http://compdiag.molgen.mpg.de/docs/talk_05_01_04_stefanie.pdf and http://en.wikipedia.org/wiki/Kernel_density_estimation . Please note that even though it contains gaussian in the name, the densities returned don't have to be normally distributed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats.kde import gaussian_kde\n",
      "import numpy as np\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as stat\n",
      "#from __future__ import division\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bonus (5 pts)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Look at the function provided below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getData(number_of_partitions,desired_test_partition):\n",
      "    data = np.genfromtxt('data.csv', delimiter=',') # Read the csv\n",
      "    \n",
      "    random.seed(31415) # Set a seed for the random generator\n",
      "    \n",
      "    hamData=data[data[:,57]==0,0:56] # Split the ham and spam data\n",
      "    spamData=data[data[:,57]==1,0:56] # Split the ham and spam data\n",
      "    data = 0\n",
      "    \n",
      "    np.random.shuffle(hamData) # Shuffle around (using the set seed)\n",
      "    np.random.shuffle(spamData)\n",
      "    \n",
      "    #Determine the range of ham messages used for testing\n",
      "    hamStartIndex=int(desired_test_partition*hamData.shape[0]/number_of_partitions)\n",
      "    hamStopIndex=int(hamStartIndex+hamData.shape[0]/number_of_partitions)\n",
      "    \n",
      "    #Determine the range of spam messages used for testing\n",
      "    spamStartIndex=int(desired_test_partition*spamData.shape[0]/number_of_partitions)\n",
      "    spamStopIndex=int(spamStartIndex+spamData.shape[0]/number_of_partitions)\n",
      "    \n",
      "    #Split the sets\n",
      "    hamTest = hamData[hamStartIndex:hamStopIndex,:]\n",
      "    hamTrain=np.delete(hamData,range(hamStartIndex,hamStopIndex),axis=0)\n",
      "    spamTest=spamData[spamStartIndex:spamStopIndex,:]\n",
      "    spamTrain=np.delete(spamData,range(spamStartIndex,spamStopIndex),axis=0)\n",
      "    return hamTrain,hamTest,spamTrain,spamTest"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Answer the following questions:\n",
      "\n",
      "What is the purpose of the number of partitions and the desired test partition? What could we use it for?\n",
      "\n",
      "TODO:\n",
      "\n",
      "Why do we use a seed for the random generator?\n",
      "\n",
      "TODO: \n",
      "\n",
      "Why do we split the ham and spam data. Why not just partition from the original set?\n",
      "\n",
      "TODO: "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 1: Probability densities for exponential distributions (2P)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "We assume each column follows an exponential distribution. Estimate the parameter $\\lambda$ for every column using the training data. Use maximum likelihood estimation to determine $\\lambda_i$. Then calculate the pdf values the following way:\n",
      "\n",
      "$pdfValues(i,j) = pdf_j(test(i,j))$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calculate_pdf_values_norm(training,test):\n",
      "    \"\"\"\n",
      "    training - a numpy array containing\n",
      "    test - a numpy array\n",
      "    \n",
      "    returns: pdf_values for the test data. This is a matrix, as specified above\n",
      "    \"\"\"\n",
      "    \n",
      "    pdf_values = np.zeros(test.shape)\n",
      "    #TODO: Estimate lambda for every column in the training data\n",
      "    \n",
      "    #TODO: Using this, determine the PDF values at the points in test and return them.\n",
      "    \n",
      "    return pdf_values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 2: Kernel Density Estimation (3P)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Your task is to use determine the values of the pdf determined by gaussian kernel density estimation (gauss_kde)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calculate_pdf_values_kde(training,test):\n",
      "    \"\"\"\n",
      "    training - a matrix containing with the training data\n",
      "    test - a matrix containing the test data\n",
      "    \n",
      "    returns: values of the pdfs for the test data (same size as the test data)\n",
      "    \"\"\"\n",
      "    pdf_values = np.zeros(test.shape)\n",
      "    #TODO: Use the following example to calculate the PDF for the test data\n",
      "    #Example:\n",
      "    sample=[1,2,2,2,3]\n",
      "    my_pdf_x = gaussian_kde(sample)\n",
      "    points=[1,1.5,2,3]\n",
      "    values=my_pdf_x(points)\n",
      "    \n",
      "    #TODO: Using these parameters, determine the coresponding PDF values for every value in test\n",
      "    return pdf_values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 3: Classifying Points (5P)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Your task is to classify the test data using the Bayes rule for both approaches. The pdf values are used for the likelihood. Use only the columns specified."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify(exponential_ham,exponential_spam,kde_ham,kde_spam,columns, T):\n",
      "    \"\"\"\n",
      "    columns is a list of columns which are to be used\n",
      "    \n",
      "    exponential_ham is a matrix contaning the pdf values of exponential distributions for the ham test data \n",
      "    \n",
      "    returns:\n",
      "    4 lists with classification results. 1 = spam, 0 = ham\n",
      "    \n",
      "    ham_labels_exponential: a list of labels for the hamTest data, based on classifying \n",
      "                         with the Bayes rule and exponential distribution assumption\n",
      "    \"\"\"\n",
      "    ham_labels_exponential = np.zeros(exponential_ham.shape[0])\n",
      "    spam_labels_exponential = np.zeros(exponential_spam.shape[0])\n",
      "    ham_labels_kde = np.zeros(kde_ham.shape[0])\n",
      "    spam_labels_kde = np.zeros(kde_spam.shape[0])\n",
      "\n",
      "    #TODO: Determine the labels by using the Bayes rule for both types\n",
      "    \n",
      "    return ham_labels_exponential, spam_labels_exponential, ham_labels_kde, spam_labels_kde"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 4: Highlight Differences (10P) "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Show the following for all columns specified in interesting_columns and both classes of data separately:\n",
      "\n",
      "A plot containing the points of the class on the x axis (use scatter), the exponential distribution estimated with the data and the pdf resulting from kernel density estimation.\n",
      "\n",
      "You are supposeed to show a total of 20 plots. Label each one with the index and class (ex. ham-1, spam-0)\n",
      "\n",
      "To visualize the pdfs you can simply determine the values on linspace (vary the interval and have plenty of points). Take care that some distributions are VERY narrow, so linspace might skip over the interesting values\n",
      "\n",
      "Make sure that the structure of the pdfs is clearly visible (parts aren't cut out and the function is "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "interesting_columns = [0,1,2,10,19,20,29,32,54,55]\n",
      "\n",
      "hamTrain,_,spamTrain,_ = getData(5,3)\n",
      "\n",
      "#TODO: Show the pdfs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: Did the assumption make sense? "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Part 5: Test the methods (10P)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the classify function, evaluate the two classifiers. Make 3D plots (for example using surface) for accuracy as a function of T and column combinations (select some column combinations and order them as you like) for both classifiers\n",
      "\n",
      "Reminder: see http://nbviewer.ipython.org/urls/raw.github.com/jrjohansson/scientific-python-lectures/master/Lecture-4-Matplotlib.ipynb for plotting"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO: Determine some column combinations. Briefly (1 sentence) explain why you chose them\n",
      "\n",
      "#TODO: Determine some values for T\n",
      "\n",
      "#TODO: Calculate the matrices for both classifiers. You only need to compute them once\n",
      "\n",
      "#TODO: Test the classifiers and plot accuracy for each of them"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    }
   ],
   "metadata": {}
  }
 ]
}